{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,torchvision,PIL,sklearn,matplotlib,wandb,captum --conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the experiment environment\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from utils.set_seed import set_seed\n",
    "from utils.data_utils import prepare_dataset, eval_dataset\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 0\n",
    "set_seed(SEED)\n",
    "\n",
    "# Get start time of the current experiment\n",
    "start_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
    "\n",
    "# Set the device to GPU if available\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "\n",
    "# Set wandb experiment name and notes\n",
    "WANDB_NAME=f\"SET EXPERIMENT NAME\"\n",
    "WANDB_NOTES=\"SET YOUR NOTE\"\n",
    "WANDB_NOTEBOOK_NAME=\"02. torch_age_inceptionv4.ipynb\"  # Change this to the name of the notebook you are running\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = WANDB_NOTEBOOK_NAME\n",
    "\n",
    "# Start a run, tracking hyperparameters\n",
    "wandb.init(\n",
    "    project=\"SET YOUR PROJECT NAME\",\n",
    "    config={\n",
    "        \"model\": \"SET YOUR MODEL NAME\", \n",
    "        \"input_size\": (299, 299),\n",
    "        \"batch_size\": 32,\n",
    "        \"augment\": True, # Set augment to True to use data augmentation\n",
    "        \"num_augmented_images\": \"Inverse of Frequency\", # If HD ImbAugment is used, set num_augmented_images to \"Inverse of Frequency\"\n",
    "        \"augment_config\": {\"augment_prob\": 1,\n",
    "                           \"flip_horizontal\": True,\n",
    "                           \"flip_vertical\": False,\n",
    "                           \"flip_prob\": 0.5,\n",
    "                           \"random_brightness\": True,\n",
    "                           \"brightness_factor\": 0.15,\n",
    "                           \"random_contrast\": True,\n",
    "                           \"contrast_factor\": 0.15,\n",
    "                           \"random_rotation\": True,\n",
    "                           \"rotation_factor\": 3,\n",
    "                           \"random_translation\": True,\n",
    "                           \"translation_factor\": (0.05, 0.05),\n",
    "                           \"random_zoom\": True,\n",
    "                           \"zoom_factors\": (0.95, 1.05),\n",
    "                           \"random_erasing\": True,\n",
    "                           \"erasing_prob\": 0.15,\n",
    "                           \"erasing_scale\": (0.05, 0.10),\n",
    "                           \"erasing_ratio\": (0.3, 3.3),\n",
    "                           }, # Set your augentation config\n",
    "        \"epochs\": 100, \n",
    "        \"learning_rate\": 0.001,\n",
    "        \"learning_rate_scheduler\": \"ReduceLROnPlateau\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"multitask\": False, # Set multitask to False to train a single task model\n",
    "        \"main_target\": \"age_in_years\",\n",
    "        \"dropout\": 0.7,\n",
    "        \"dense_units\": 1024,\n",
    "        \"early_stopping\": True,\n",
    "        \"early_stopping_patience\": 20,\t\n",
    "    },\n",
    "    notes=WANDB_NOTES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from csv file\n",
    "\n",
    "df = pd.read_csv(\"Data\\ccs_dataset.csv\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train, validation and test sets with scikit-learn\n",
    "# IMPORTANT: If using ImbAug set same seed and split size as used in pre-processing to avoid data leakage.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df.age_group)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df.age_group)\n",
    "\n",
    "# Deleting temp_df to free memory\n",
    "del temp_df\n",
    "\n",
    "# # If using HD ImbAugment, delete train_df to free memory and load the augmented dataset\n",
    "if wandb.config.num_augmented_images == \"Inverse of Frequency\":\n",
    "    del train_df\n",
    "    train_df = pd.read_csv(\"path/to/augmented/train/dataset/generated/on/notebook/01.csv\"))\n",
    "    train_df.head(10).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing datasets\n",
    "\n",
    "if wandb.config.num_augmented_images == \"Inverse of Frequency\":\n",
    "    train_dataset = prepare_dataset(\n",
    "        train_df,\n",
    "        input_size=wandb.config.input_size,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True,\n",
    "        augment=False,\n",
    "        multitask=wandb.config.multitask,\n",
    "        label=wandb.config.main_target,\n",
    "        augment_config=wandb.config.augment_config,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    train_dataset = prepare_dataset(\n",
    "        train_df,\n",
    "        input_size=wandb.config.input_size,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True,\n",
    "        augment=wandb.config.augment,\n",
    "        num_augmented_images=wandb.config.num_augmented_images,\n",
    "        multitask=wandb.config.multitask,\n",
    "        label=wandb.config.main_target,\n",
    "        augment_config=wandb.config.augment_config,\n",
    "    )\n",
    "\n",
    "val_dataset = prepare_dataset(\n",
    "    val_df,\n",
    "    input_size=wandb.config.input_size,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    multitask=wandb.config.multitask,\n",
    "    label=wandb.config.main_target,\n",
    ")\n",
    "\n",
    "test_dataset = prepare_dataset(\n",
    "    test_df,\n",
    "    input_size=wandb.config.input_size,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    shuffle=False,\n",
    "    augment=False,\n",
    "    multitask=wandb.config.multitask,\n",
    "    label=wandb.config.main_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset was loaded correctly\n",
    "\n",
    "eval_dataset(train_dataset, multitask=wandb.config.multitask, label=wandb.config.main_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from models.pytorch.architectures.InceptionV4 import (\n",
    "    InceptionStem,\n",
    "    InceptionA,\n",
    "    InceptionB,\n",
    "    InceptionC,\n",
    "    ReductionA,\n",
    "    ReductionB,\n",
    ")\n",
    "from utils.training_utils import weights_init\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class InceptionV4(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_prob, dense_units):\n",
    "        super(InceptionV4, self).__init__()\n",
    "\n",
    "        self.stem = InceptionStem()\n",
    "\n",
    "        self.inception_a_blocks = nn.Sequential(\n",
    "            InceptionA(384),\n",
    "            InceptionA(384),\n",
    "            InceptionA(384),\n",
    "            InceptionA(384),\n",
    "        )\n",
    "\n",
    "        self.reduction_a = ReductionA(384)\n",
    "\n",
    "        self.inception_b_blocks = nn.Sequential(\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "            InceptionB(1024),\n",
    "        )\n",
    "\n",
    "        self.reduction_b = ReductionB(1024)\n",
    "\n",
    "        self.inception_c_blocks = nn.Sequential(\n",
    "            InceptionC(1536),\n",
    "            InceptionC(1536),\n",
    "            InceptionC(1536),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(1536, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_a_blocks(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_b_blocks(x)\n",
    "        x = self.reduction_b(x)\n",
    "        x = self.inception_c_blocks(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = InceptionV4(\n",
    "    num_classes=1,\n",
    "    dropout_prob=wandb.config.dropout,\n",
    "    dense_units=wandb.config.dense_units,\n",
    ").to(DEVICE).apply(weights_init)\n",
    "\n",
    "\n",
    "summary(model, (3, wandb.config.input_size[0], wandb.config.input_size[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "mse_metric = nn.MSELoss()\n",
    "\n",
    "# Define scaler to use mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Tunable training parameters\n",
    "# Epochs\n",
    "epochs = wandb.config.epochs\n",
    "\n",
    "# Optimizer\n",
    "if wandb.config.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "elif wandb.config.optimizer == \"RMSprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "if wandb.config.learning_rate_scheduler == \"ReduceLROnPlateau\":\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "from utils.training_utils import age_train_loop, age_valid_loop\n",
    "\n",
    "train_losses = []\n",
    "train_MSEs = []\n",
    "valid_losses = []\n",
    "valid_MSEs = []\n",
    "early_stopping_patience = wandb.config.early_stopping_patience\n",
    "best_valid_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(\n",
    "        f\"-------------------------------\\nEpoch {t+1}\\n-------------------------------\"\n",
    "    )\n",
    "    train_loss, train_mse = age_train_loop(\n",
    "        train_dataset, model, criterion, mse_metric, optimizer, device=DEVICE, scaler=scaler\n",
    "    )\n",
    "    valid_loss, valid_mse = age_valid_loop(\n",
    "        test_dataset, model, criterion, mse_metric, device=DEVICE\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_MSEs.append(train_mse)\n",
    "    valid_MSEs.append(valid_mse)\n",
    "\n",
    "    print(\n",
    "        \"\\nMetrics -> \"\n",
    "        f\"loss: {train_loss:>5f} | \"\n",
    "        f\"MSE: {train_mse:>5f} | \"\n",
    "        f\"val_loss: {valid_loss:>5f} | \"\n",
    "        f\"val_MSE: {valid_mse:>5f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']}\"\n",
    "    )\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch/epoch\": t,\n",
    "            \"epoch/mae\": train_loss,\n",
    "            \"epoch/mse\": train_mse,\n",
    "            \"epoch/val_mae\": valid_loss,\n",
    "            \"epoch/val_mse\": valid_mse,\n",
    "            \"epoch/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    # Early Stopping and Save Best Model logic\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "\n",
    "        # Save the best model\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"models/pytorch/weights/best_{WANDB_NAME}.pt\",\n",
    "        )\n",
    "        print(\"\\nBest model saved.\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"\\nEpochs without improvement: {epochs_without_improvement}\")\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"\\nEarly stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(valid_losses, label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation MSEs\n",
    "\n",
    "plt.plot(train_MSEs, label=\"Train MSE\")\n",
    "plt.plot(valid_MSEs, label=\"Valid MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model with best weights on test dataset\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"models/pytorch/weights/best_{WANDB_NAME}.pt\",\n",
    "        map_location=torch.device(DEVICE),\n",
    "    )\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_mse = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataset:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.view(-1), labels.float())\n",
    "        mse = mse_metric(outputs.view(-1, 1), labels.view(-1, 1))\n",
    "        total_loss += loss.item()\n",
    "        total_mse += mse.item()\n",
    "        total += 1\n",
    "\n",
    "avg_test_loss = total_loss / total\n",
    "avg_test_mse = total_mse / total\n",
    "print(f\"Average test loss: {avg_test_loss}\")\n",
    "print(f\"Average test MSE: {avg_test_mse}\")\n",
    "\n",
    "# Log the test metrics to wandb\n",
    "wandb.log({\"test_mae\": avg_test_loss, \"test_mse\": avg_test_mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add test results to summary and finish the experiment\n",
    "\n",
    "wandb.run.summary[\"model_saved\"] = f'best_{WANDB_NAME}.pt'\n",
    "wandb.run.summary[\"test_mae\"] = avg_test_loss\n",
    "wandb.run.summary[\"test_mse\"] = avg_test_mse\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
