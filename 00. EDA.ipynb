{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study: Deep learning in the estimation of chronological age and biological sex using panoramic radiographs\n",
    "# Author: Willian Oliveira\n",
    "# Start: 31/03/2023\n",
    "# Motivation: An exploratory data analysis of odontological panoramic radiographs dataset.\n",
    "# Study Status: Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from IPython.display import display, Markdown\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# Silence warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image metadata\n",
    "\n",
    "id_df = pd.read_excel('PATH_TO_EXCEL_FILE')\n",
    "id_df.rename(columns={'idade_meses': 'age_in_months'}, inplace=True)\n",
    "id_df.drop(columns=['age_in_months'], inplace=True)\n",
    "id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading image metadata\n",
    "\n",
    "id_df2 = pd.read_csv('PATH_TO_CSV_FILE')\n",
    "id_df2.drop(columns=['PanoURL'], inplace=True)\n",
    "id_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicated ids between the two datasets\n",
    "\n",
    "id_df2[id_df2['id'].isin(id_df['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset sizes\n",
    "\n",
    "print(f'First dataset size: {id_df.shape[0]}')\n",
    "print(f'Second dataset size: {id_df2.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "id_df = pd.concat([id_df, id_df2], ignore_index=True)\n",
    "\n",
    "# Check dataset size\n",
    "print(f'Concatenated dataset size: {id_df.shape[0]}')\n",
    "id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string age to float\n",
    "def convert_age_to_years(age_str):\n",
    "    try:\n",
    "        years, months = age_str.split(\" anos e \")\n",
    "        years = int(years)\n",
    "        months = int(months.strip()[:-6])\n",
    "        total_years = round((years + (months / 12)), 2)\n",
    "        return total_years\n",
    "    except ValueError as e:\n",
    "        print(f\"Error on '{age_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "id_df['age_in_years'] = id_df['age'].apply(convert_age_to_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Path to the exam image on dataframe\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('PATH_TO_PANORAMICS', '*.jpg'))}\n",
    "\n",
    "print('PXR found:', len(all_image_paths), ', Total Headers', id_df.shape[0])\n",
    "\n",
    "id_df['id_with_extension'] = id_df['id'].apply(lambda x: f'{x}.jpg')\n",
    "id_df['path'] = id_df['id_with_extension'].map(all_image_paths.get)\n",
    "id_df = id_df[['id', 'path', 'sex', 'age_in_years']]\n",
    "\n",
    "id_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there are repeated ids\n",
    "\n",
    "print(f\"There are {id_df['id'].duplicated().sum()} repeated ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Hash identifier of the pacient\n",
    "# creatre a list to store the data\n",
    "data = []\n",
    "\n",
    "# Open the file and read the lines\n",
    "with open('PATH_TO_PATIENTS_ID', 'r') as f:\n",
    "    for line in f:\n",
    "        # Ignore blank lines\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "\n",
    "        # Create a dictionary to store the data\n",
    "        dict_data = {}\n",
    "\n",
    "        # Search for 'id' information in the line and store it in the dictionary\n",
    "        id_match = re.search(\"'id': '([^']+)\", line)\n",
    "        if id_match:\n",
    "            dict_data['id'] = id_match.group(1)\n",
    "\n",
    "        # Create a hashlib object\n",
    "        m = hashlib.sha256()\n",
    "\n",
    "        # First try to find the email\n",
    "        email_match = re.search(\"'email': '([^']+)\", line)\n",
    "        if email_match:\n",
    "            m.update(email_match.group(1).encode())\n",
    "        else:  # If no email is found, try to find the name\n",
    "            name_match = re.search(\"'name': '([^']+)\", line)\n",
    "            if name_match:\n",
    "                m.update(name_match.group(1).encode())\n",
    "\n",
    "        dict_data['p_hash'] = m.hexdigest()\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        data.append(dict_data)\n",
    "\n",
    "# Convert the list to a Pandas DataFrame\n",
    "hash_df = pd.DataFrame(data)\n",
    "\n",
    "# Set variables as string\n",
    "hash_df = hash_df.astype(str)\n",
    "\n",
    "# Drop nan IDs\n",
    "hash_df.dropna(subset=['id'], inplace=True)\n",
    "\n",
    "# Check duplicated ids\n",
    "\n",
    "if hash_df['id'].duplicated().sum() > 0:\n",
    "    display(Markdown(\"### Duplicated ids\"))\n",
    "    display(hash_df[hash_df['id'].duplicated(keep=False)].sort_values(by='id').style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables as string\n",
    "id_df['id'] = id_df.id.astype(str)\n",
    "\n",
    "# Merge hash_df with id_df\n",
    "id_df = id_df.merge(hash_df, on='id', how='left')\n",
    "\n",
    "# Check the DataFrame\n",
    "print(f\"There are {id_df['id'].duplicated().sum()} duplicated ids.\")\n",
    "print(f\"There are {id_df['p_hash'].duplicated().sum()} duplicated hashes.\")\n",
    "print(f\"There are {id_df['id'].nunique()} unique ids.\")\n",
    "print(f\"There are {id_df['p_hash'].nunique()} unique hashes.\")\n",
    "print(f\"There are {id_df['p_hash'].isna().sum()} missing hashes.\")\n",
    "\n",
    "# Show a sample of DataFrame\n",
    "id_df.head().style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan hashes\n",
    "\n",
    "id_df.dropna(subset=['p_hash'], inplace=True)\n",
    "dup_id_df = id_df[id_df['p_hash'].duplicated(keep=False)]\n",
    "\n",
    "# Check the DataFrame\n",
    "print(f\"There are {id_df['id'].duplicated().sum()} duplicated ids.\")\n",
    "print(f\"There are {dup_id_df['p_hash'].nunique()} duplicated hashes.\")\n",
    "print(f\"There are {dup_id_df.shape[0]} exams with repeated hashes.\")\n",
    "print()\n",
    "print(f\"There are {id_df['id'].nunique()} unique ids.\")\n",
    "print(f\"There are {id_df['p_hash'].nunique()} unique hashes.\")\n",
    "print(f\"There are {id_df['p_hash'].isna().sum()} missing hashes.\")\n",
    "\n",
    "# Show a sample of DataFrame\n",
    "id_df.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify duplicated hashes:\n",
    "\n",
    "print(dup_id_df.shape)\n",
    "print(dup_id_df['p_hash'].nunique())\n",
    "\n",
    "dup_id_df.head().style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated hashes\n",
    "\n",
    "id_df.drop_duplicates(subset=['p_hash'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are duplicated hashes\n",
    "\n",
    "print(f\"There are {id_df['p_hash'].duplicated().sum()} duplicated hashes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the image shape for the non-corrupted images to verify if there is inconsistency.\n",
    "\n",
    "def get_image_shape(image_path):\n",
    "    '''\n",
    "    Check if image is corrupted and return explicit error message to help debugging if so.\n",
    "    It also returns the image shape for the non-corrupted images.\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        return img.size\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        return None\n",
    "\n",
    "id_df[\"image_shape\"] = id_df[\"path\"].apply(get_image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many images have the same shape\n",
    "\n",
    "id_df[['path', 'image_shape']].groupby('image_shape').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify images with diferent shapes\n",
    "\n",
    "display(id_df[id_df['image_shape'] == (596, 474)])\n",
    "display(Image.open('data/panoramics/122606.jpg'))\n",
    "\n",
    "display(id_df[id_df['image_shape'] == (474, 596)])\n",
    "display(Image.open('data/panoramics/136554.jpg'))\n",
    "\n",
    "display(id_df[id_df['image_shape'] == (960, 768)])\n",
    "display(Image.open('data/panoramics/95298.jpg'))\n",
    "\n",
    "display(id_df[id_df['image_shape'] == (1163, 1600)])\n",
    "display(Image.open('data/panoramics/130771.jpg'))\n",
    "\n",
    "display(id_df[id_df['image_shape'] == (4781, 3781)])\n",
    "display(Image.open('data/panoramics/137704.jpg'))\n",
    "display(Image.open('data/panoramics/135025.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "\n",
    "def display_missing_values(df):\n",
    "    \"\"\"\n",
    "    This function calculates the percentage of missing values in each column of a\n",
    "    Pandas DataFrame and displays the results in descending order. \n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    missing_values = df.isnull().sum() / len(df) * 100\n",
    "    missing_values = missing_values.sort_values(ascending=False)\n",
    "    missing_values.rename(\"% Missing Values\", inplace=True)\n",
    "    display(Markdown(missing_values.to_markdown()))\n",
    "    del missing_values\n",
    "\n",
    "display_missing_values(id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking feature unique values\n",
    "\n",
    "def unique_values_table(df, uv=3):\n",
    "    \"\"\"\n",
    "    Print a markdown table\n",
    "    with the col, the number of unique values and the unique values \n",
    "    list if there are less than 3 unique values (uv) by defalt.\n",
    "\n",
    "    :param uv: int\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    from IPython.display import display, Markdown\n",
    "    md_table_str = '|Column Name|Unique Values||\\n|---|---|---|\\n'\n",
    "    for col_name, unique_values in df.nunique().items():\n",
    "        if unique_values > uv:\n",
    "            md_table_str += '|{}|{}|\\n'.format(col_name, unique_values)\n",
    "        else:\n",
    "            md_unique_str = ' '.join([\n",
    "                f'{name}: {value*100:.2f}\\%'\n",
    "                for name, value in \n",
    "                df[col_name].value_counts(normalize=True).items()\n",
    "            ])\n",
    "\n",
    "            md_table_str += '|{}|{}|{}\\n'.format(\n",
    "                col_name, unique_values, md_unique_str)\n",
    "    display(Markdown(md_table_str))\n",
    "\n",
    "unique_values_table(id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop corrupted images\n",
    "\n",
    "id_df.dropna(subset=['image_shape'], inplace=True)\n",
    "\n",
    "# Drop cropped images, unexpected and unprocessed images\n",
    "\n",
    "id_df.drop(id_df[id_df['image_shape'] == (596, 474)].index, inplace=True)\n",
    "id_df.drop(id_df[id_df['image_shape'] == (474, 596)].index, inplace=True)\n",
    "id_df.drop(id_df[id_df['image_shape'] == (960, 768)].index, inplace=True)\n",
    "id_df.drop(id_df[id_df['image_shape'] == (1163, 1600)].index, inplace=True)\n",
    "id_df.drop(id_df[id_df['image_shape'] == (4781, 3781)].index, inplace=True)\n",
    "\n",
    "# Checking basic statistics\n",
    "\n",
    "id_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hash for each image to be used as a unique identifier of the image\n",
    "\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def get_image_hash(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img_byte_arr = io.BytesIO()\n",
    "            img.save(img_byte_arr, format='JPEG')\n",
    "            img_byte_arr = img_byte_arr.getvalue()\n",
    "            hash_object = hashlib.sha256(img_byte_arr)\n",
    "        return hash_object.hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process image at {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "id_df['i_hash'] = id_df['path'].apply(get_image_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if inclusion of hash column was successful\n",
    "print(f\"Dataset shape: {id_df.shape}\")\n",
    "id_df.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any duplicate images using the hash\n",
    "\n",
    "print(f\"Number of unique images: {id_df['i_hash'].nunique()}\")\n",
    "print(f\"Number of duplicate images: {id_df.shape[0] - id_df['i_hash'].nunique()}\")\n",
    "\n",
    "# Print duplicate images\n",
    "\n",
    "id_df[id_df.duplicated(subset=['i_hash'], keep=False)].sort_values(by=['i_hash']).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_df['sex'] = id_df['sex'].map({0:'M', 1: 'F'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify age distribution among the patients\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = sns.color_palette(\"pastel\")\n",
    "sns.histplot(data=id_df, x='age_in_years', hue='sex', kde=True, multiple=\"stack\")\n",
    "#plt.title('Age distribution among the patients by gender')\n",
    "plt.xlabel('Age in years', fontsize=15, labelpad=15)\n",
    "plt.ylabel('Number of Patients', fontsize=15, labelpad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop patients with age == 0.00\n",
    "id_df = id_df[id_df['age_in_years'] != 0.00]\n",
    "\n",
    "print(f\"Mean age: {id_df['age_in_years'].mean():.2f}\")\n",
    "print(f\"Median age: {id_df['age_in_years'].median():.2f}\")\n",
    "print(f\"Mode age: {id_df['age_in_years'].mode()[0]:.2f}\")\n",
    "print(f\"Standard Deviation of age: {id_df['age_in_years'].std():.2f}\")\n",
    "print(f\"Age range: {id_df['age_in_years'].min():.2f} to {id_df['age_in_years'].max():.2f}\")\n",
    "\n",
    "#Distribution of patients by sex: [Insert number] Male, [Insert number] Female\n",
    "# Count patients by sex\n",
    "\n",
    "num_female_patients = id_df[id_df['sex'] == 1].shape[0]\n",
    "num_male_patients = id_df[id_df['sex']== 0].shape[0]\n",
    "\n",
    "print(f'Male: {num_male_patients}, Female: {num_female_patients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group patients by age in groups of 5 years\n",
    "\n",
    "id_df['age_group'] = pd.cut(id_df['age_in_years'], bins=range(0, 101, 5), right=False)\n",
    "id_df['age_group'] = id_df['age_group'].astype(str)\n",
    "\n",
    "id_df['age_group'] = np.where(id_df['age_in_years'] >= 90, '[90, 100)', id_df['age_group'])\n",
    "\n",
    "id_df[['age_group', 'age_in_years']].groupby('age_group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sex to binary values\n",
    "id_df['sex'] = id_df['sex'].map({'M': 0, 'F': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the treated dataframe to a csv file\n",
    "\n",
    "id_df = id_df[['id', 'path', 'p_hash', 'i_hash', 'sex', 'age_in_years', 'age_group']]\n",
    "id_df.to_csv('data/ccs_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
